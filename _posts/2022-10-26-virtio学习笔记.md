---
layout: post
title: Welcome to Not Pure Poole
date: 2020-09-29 23:18 +0800
last_modified_at: 2020-10-01 01:08:25 +0800
tags: [jekyll theme, jekyll, tutorial]
toc:  true
---
Welcome to **Not Pure Poole**! This is an example post to show the layout.
{: .message }

# virtio学习笔记

# 框架分析

## 1、概述

完全虚拟化情况下，虚拟机内部的设备完全不知道自己处于虚拟化环境中，所以IO操作会完整走：虚拟机内核栈->QEMU->宿主机内核栈，从而产生了许多VM Exit和VM Entry，导致性能很差。Virtio方案旨在提高I/O性能。在该方案中虚拟机能够感知到自己处于虚拟化环境中（半虚拟化），并且会加载相应的virtio总线驱动和virtio设备驱动，执行自己定义的 协议进行数据传输，减少VM Exit和VM Entry操作。

## 2、架构

VirtIO由 Rusty Russell 开发，是虚拟化 hypervisor 中的一组通用模拟设备IO的抽象。Virtio是一种前后端架构，包括前端驱动（Guest内部）、后端设备（QEMU设备）、传输协议（vring）。框架如下图所示：

![img](/../virtio学习笔记.assets/431521-20180118130745928-1245740398.png)

- 前端驱动

虚拟机内部的 virtio模拟设备对应的驱动。作用为接收用户态的请求，然后按照传输协议对请求进行封装，再写I/O操作，发送通知到QEMU后端设备。

- 后端驱动

在QEMU中创建，用来接收前端驱动发送的I/O请求，然后按照传输协议进行解析，在对物理设备进行操作，之后通过中断机制通知前端设备。

- 传输协议

​       使用virtio队列（virtio queue，virtqueue）完成。设备有若干个队列，每个队列处理不同的数据传输（如virtio-balloon包含ivq、dvq、svq三个）。
​        virtqueue通过vring实现。Vring是虚拟机和QEMU之间共享的一段环形缓冲区，QEMU和前端设备都可以从vring中读取数据和放入数据。

## 3、原理

### 3.1 总体流程

从代码上看，virtio的代码主要分两个部分：QEMU和内核驱动程序。Virtio设备的模拟就是通过QEMU完成的，QEMU代码在虚拟机启动之前，创建虚拟设备。虚拟机启动后检测到设备，调用内核的virtio设备驱动程序来加载这个virtio设备。

  对于KVM虚拟机，都是通过QEMU这个用户空间程序创建的，每个KVM虚拟机都是一个QEMU进程，虚拟机的virtio设备是QEMU进程模拟的，虚拟机的内存也是从QEMU进程的地址空间内分配的。

  VRING是由虚拟机virtio设备驱动创建的用于数据传输的共享内存，QEMU进程通过这块共享内存获取前端设备递交的IO请求。

  如下图所示，虚拟机IO请求的整个流程：

![img](/../virtio学习笔记.assets/774036-20211126153504501-1486553797.png)

-  虚拟机产生的IO请求会被前端的virtio设备接收，并存放在virtio设备散列表scatterlist里；
- Virtio设备的virtqueue提供add_buf函数将散列表中的数据映射至前后端数据共享区域Vring中；
- Virtqueue通过kick函数来通知后端qemu进程。Kick通过写pci配置空间的寄存器产生kvm_exit；
- Qemu端注册ioport_write/read函数监听PCI配置空间的改变，获取前端的通知消息；
- Qemu端维护的virtqueue队列从数据共享区vring中获取数据，通过get_buf这个API获取；
- Qemu将数据封装成virtioreq;
- Qemu进程将请求发送至硬件层。

前后端主要通过PCI配置空间的寄存器完成前后端的通信，而IO请求的数据地址则存在vring中，并通过共享vring这个区域来实现IO请求数据的共享。从上图中可以看到，Virtio设备的驱动分为前端与后端：前端是虚拟机的设备驱动程序，后端是host上的QEMU用户态程序。为了实现虚拟机中的IO请求从前端设备驱动传递到后端QEMU进程中，Virtio框架提供了两个核心机制：前后端消息通知机制和数据共享机制。

- **消息通知机制**

前端驱动设备产生IO请求后，可以通知后端QEMU进程去获取这些IO请求，递交给硬件。

- **数据共享机制**

前端驱动设备在虚拟机内申请一块内存区域，将这个内存区域共享给后端QEMU进程，前端的IO请求数据就放入这块共享内存区域，QEMU接收到通知消息后，直接从共享内存取数据。由于KVM虚拟机就是一个QEMU进程，虚拟机的内存都是QEMU申请和分配的，属于QEMU进程的线性地址的一部分，因此虚拟机只需将这块内存共享区域的地址传递给QEMU进程，QEMU就能直接从共享区域存取数据。

**疑问**：这里应该需要做一下地址转换吧，把虚拟机的物理地址转为宿主机的虚拟地址吧？？？

### 3.2 PCI配置空间

由整体流程图可知，guest和host交互传送信息的两个重要结构分别的PCI config和vring，本节重点分析实现消息通知机制的PCI配置空间。

#### 3.2.1 虚拟机是如何获取PCI配置空间？

首先，QEMU为虚拟机创建的virtio设备都是PCI设备，它们挂在PCI总线上，遵循通用PCI设备的发现、挂载等机制。

当虚拟机启动发现virtio PCI设备时，只有配置空间可以被访问，配置空间内保存着该设备工作所需的信息，如厂家、功能、资源要求等，通过对这个空间信息的读取，完成对PCI设备的配置。同时配置空间上有一块**寄存器空间**，里面包含了一些寄存器和IO空间。

**前后端的通知消息就是写在寄存器空间的寄存器**，virtio会为它的PCI设备注册一个PCI BAR来访问这块**寄存器空间**。配置空间如下图所示：

![image-20220319153531266](/../virtio学习笔记.assets/image-20220319153531266.png)

虚拟机系统在启动过程中在PCI总线上发现virtio-pci设备，就会调用virtio-pci的probe函数。**该函数会将PCI配置空间上的寄存器空间的首地址映射到内存空间，并将这个地址赋值给virtio_pci_device的ioaddr变量。之后要对PCI配置空间上的寄存器操作时，只需要ioaddr+偏移量。**

映射示例代码如下：

```c
vp_dev->ioaddr = pci_iomap(pci_dev, 0, 0);
```

pci_iomap函数完成PCI BAR的映射，第一个参数是pci设备的指针，第二个参数指定我们要映射的是0号BAR，第三个参数确定要映射的BAR空间多大，当第三个参数为0时，就将整个0号BAR空间都映射到内存空间上。VirtioPCI设备的0号BAR指向的就是**配置空间的寄存器空间**，也就是配置空间上用于消息通知的寄存器。

通过pci_iomap之后，我们就可以像操作普通内存一样（调用ioread和iowrite）来读写pci硬件设备上的寄存器。

#### 3.2.2 虚拟机如何操作这个配置空间？

- kick函数

当前端设备的驱动程序需要通知后端QEMU程序执行某些操作的时候，就会调用kcik函数，来触发读写PCI配置空间寄存器的动作。

- 读写PCI寄存器

ioread/iowrite实现了对配置空间寄存器的读写，例如：

```c
/* Select the queue we're interested in */
iowrite16(index, vp_dev->ioaddr + VIRTIO_PCI_QUEUE_SEL);

/* Check if queue is either not available or already active. */
num = ioread16(vp_dev->ioaddr + VIRTIO_PCI_QUEUE_NUM);
```

这里的内存地址都是通过ioaddr加上偏移实现的（字节为单位），相关偏移的宏定义如下所示：

```c
/* A 32-bit r/o bitmask of the features supported by the host */
#define VIRTIO_PCI_HOST_FEATURES	0
/* A 32-bit r/w bitmask of features activated by the guest */
#define VIRTIO_PCI_GUEST_FEATURES	4
/* A 32-bit r/w PFN for the currently selected queue */
#define VIRTIO_PCI_QUEUE_PFN		8
/* A 16-bit r/o queue size for the currently selected queue */
#define VIRTIO_PCI_QUEUE_NUM		12
/* A 16-bit r/w queue selector */
#define VIRTIO_PCI_QUEUE_SEL		14
/* A 16-bit r/w queue notifier */
#define VIRTIO_PCI_QUEUE_NOTIFY		16
```

#### **3.2.3 qemu如何感知虚拟机的操作的？**

​		虚拟机内调用kick函数实现通知之后，会产生KVM_EXIT。Host端的kvm模块捕获到这个EXIT之后，根据它退出的原因来做处理。如果是一个IO_EXIT，kvm会将这个退出交给用户态的QEMU程序来完成IO操作。
　　QEMU为kvm虚拟机模拟了virtio设备，因此**后端的virtio-pci设备**也是在QEMU进程中模拟生成的。QEMU对**模拟的PCI设备的配置空间注册了回调函数**，当虚拟机产生IO_EXIT，就调用这些函数来处理事件。

legacy模式：

- 监听PCI寄存器

​        virtio_ioport_write/read就是QEMU进程监听PCI配置空间上寄存器消息的函数，针对前端iowrite/ioread读写了哪个PCI寄存器，来决定下一步操作：

```c
static void virtio_ioport_write(void *opaque, uint32_t addr, uint32_t val)
{
    VirtIOPCIProxy *proxy = opaque;
    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);
    hwaddr pa;

    switch (addr) {
    case VIRTIO_PCI_GUEST_FEATURES:
        /* Guest does not negotiate properly?  We have to assume nothing. */
        if (val & (1 << VIRTIO_F_BAD_FEATURE)) {
            val = virtio_bus_get_vdev_bad_features(&proxy->bus);
        }
        virtio_set_features(vdev, val);
        break;
    case VIRTIO_PCI_QUEUE_PFN:
        pa = (hwaddr)val << VIRTIO_PCI_QUEUE_ADDR_SHIFT;
        if (pa == 0) {
            virtio_pci_reset(DEVICE(proxy));
        }
        else
            virtio_queue_set_addr(vdev, vdev->queue_sel, pa);
        break;
    case VIRTIO_PCI_QUEUE_SEL:
        if (val < VIRTIO_QUEUE_MAX)
            vdev->queue_sel = val;
        break;
    case VIRTIO_PCI_QUEUE_NOTIFY:
        if (val < VIRTIO_QUEUE_MAX) {
            virtio_queue_notify(vdev, val);
        }
        break;
    case VIRTIO_PCI_STATUS:
        if (!(val & VIRTIO_CONFIG_S_DRIVER_OK)) {
            virtio_pci_stop_ioeventfd(proxy);
        }

        virtio_set_status(vdev, val & 0xFF);

        if (val & VIRTIO_CONFIG_S_DRIVER_OK) {
            virtio_pci_start_ioeventfd(proxy);
        }

        if (vdev->status == 0) {
            virtio_pci_reset(DEVICE(proxy));
        }

        /* Linux before 2.6.34 drives the device without enabling
           the PCI device bus master bit. Enable it automatically
           for the guest. This is a PCI spec violation but so is
           initiating DMA with bus master bit clear. */
        if (val == (VIRTIO_CONFIG_S_ACKNOWLEDGE | VIRTIO_CONFIG_S_DRIVER)) {
            pci_default_write_config(&proxy->pci_dev, PCI_COMMAND,
                                     proxy->pci_dev.config[PCI_COMMAND] |
                                     PCI_COMMAND_MASTER, 1);
        }
        break;
    case VIRTIO_MSI_CONFIG_VECTOR:
        msix_vector_unuse(&proxy->pci_dev, vdev->config_vector);
        /* Make it possible for guest to discover an error took place. */
        if (msix_vector_use(&proxy->pci_dev, val) < 0)
            val = VIRTIO_NO_VECTOR;
        vdev->config_vector = val;
        break;
    case VIRTIO_MSI_QUEUE_VECTOR:
        msix_vector_unuse(&proxy->pci_dev,
                          virtio_queue_vector(vdev, vdev->queue_sel));
        /* Make it possible for guest to discover an error took place. */
        if (msix_vector_use(&proxy->pci_dev, val) < 0)
            val = VIRTIO_NO_VECTOR;
        virtio_queue_set_vector(vdev, vdev->queue_sel, val);
        break;
    default:
        qemu_log_mask(LOG_GUEST_ERROR,
                      "%s: unexpected address 0x%x value 0x%x\n",
                      __func__, addr, val);
        break;
    }
}
```

- 监听函数注册

PCI寄存器的这些监听函数，都是在QEMU为虚拟机创建虚拟设备的时候注册。QEMU先为虚拟机的virtio-pci设备创建PCI配置空间，配置空间内包含了设备的一些基本信息；在配置空间的存储空间位置注册了一个PCI BAR，并为这个BAR注册了回调函数监听寄存器的改变。

这部分代码是初始化配置空间的基本信息。示例代码如下：

```c
if (legacy) {
    size = VIRTIO_PCI_REGION_SIZE(&proxy->pci_dev)
        + virtio_bus_get_vdev_config_len(bus);
    size = pow2ceil(size);

    memory_region_init_io(&proxy->bar, OBJECT(proxy),
                          &virtio_pci_config_ops,
                          proxy, "virtio-pci", size);

    pci_register_bar(&proxy->pci_dev, proxy->legacy_io_bar_idx,
                     PCI_BASE_ADDRESS_SPACE_IO, &proxy->bar);
}
```

​		给PCI设备注册了PCI BAR，指定起始地址为**PCI_BASE_ADDRESS_SPACE_IO**（即PCI配置空间中存储空间到配置空间首地址的偏移值）；

　　指定这个BAR的大小为size，回调函数为virtio_pci_config_ops中的读写函数。

```c
static const MemoryRegionOps virtio_pci_config_ops = {
    .read = virtio_pci_config_read,
    .write = virtio_pci_config_write,
    .impl = {
        .min_access_size = 1,
        .max_access_size = 4,
    },
    .endianness = DEVICE_LITTLE_ENDIAN,
};
```

这里的read/write最终都会调用virtio_ioport_write（virtio_ioport_write处理前端写寄存器时触发的事件，virtio_ioport_read处理前端要读寄存器时触发的事件）来统一的管理。

### 3.3 前后端数据共享

#### **3.3.1 Vring数据结构**

```c
struct vring {
    unsigned int num;
    struct vring_desc *desc;
    struct vring_avail *avail;
    struct vring_used *used;
};
```

　　VRING共享区域总共有三个表：

　　**vring_desc**表，存放虚拟机产生的IO请求的地址；

　　**vring_avail**表，指明vring_desc中哪些项是可用的；

　　**vring_used**表,   指明vring_desc中哪些项已经被递交到硬件。

这样，我们往virng_desc表中存放IO请求，用**vring_avail告诉QEMU进程vring_desc表中哪些项是可用的**，QEMU将IO请求递交给硬件执行后，**用vring_used表来告诉前端vring_desc表中哪些项已经被递交**，可以释放这些项了。

- vring_desc

```c
/* Virtio ring descriptors: 16 bytes.  These can chain together via "next". */
struct vring_desc {
    /* Address (guest-physical). */
    __virtio64 addr;
    /* Length. */
    __virtio32 len;
    /* The flags as indicated above. */
    __virtio16 flags;
    /* We chain unused descriptors via this, too */
    __virtio16 next;
};
```

存储虚拟机产生的IO请求在内存中的地址(GPA地址)，在这个表中每一行都包含四个域，如下所示：

- **Addr，**存储IO请求在虚拟机内的内存地址，是一个GPA值；

- **len，**表示这个IO请求在内存中的长度；

- **flags，**指示这一行的数据是可读、可写（VRING_DESC_F_WRITE），是否是一个请求的最后一项（VRING_DESC_F_NEXT）；

- **next，**每个IO请求都有可能包含了vring_desc表中的多行，next域就指明了这个请求的下一项在哪一行。

　　其实，通过next我们就将一个IO请求在vring_desc中存储的多行连接成了一个链表，当flag=~ VRING_DESC_F_NEXT，就表示这个链表到了末尾。

　　如下图所示，表示desc表中有两个IO请求，分别通过next域组成了链表。

![image-20220319162834444](/../virtio学习笔记.assets/image-20220319162834444.png)

- vring_avail

存储的是每个IO请求在vring_desc中连接成的链表的表头位置。数据结构如下所示：

```c
struct vring_avail {
    __virtio16 flags;
    __virtio16 idx;
    __virtio16 ring[];
};
```

　　在vring_desc表中：

- **ring[]**, 通过next域连接起来的链表的表头在vring_desc表中的位置

- **idx**，指向的是ring数组中下一个可用的空闲位置；

- **flags**是一个标志域。

如下图所示， vring_avail表指明了vring_desc表中有两个IO请求组成的链表是最近更新可用的，它们分别从0号位置和3号位置开始。

![image-20220520162642772](virtio学习笔记.assets\image-20220520162642772.png)

- vring_used

```c
struct vring_used_elem {
    /* Index of start of used descriptor chain. */
    __virtio32 id;
    /* Total length of the descriptor chain which was used (written to) */
    __virtio32 len;
};

struct vring_used {
    __virtio16 flags;
    __virtio16 idx;
    struct vring_used_elem ring[];
};
```

- idx    指向了ring数组中下一个可用的位置；
- flags 是标记位。
- ring[] 这是一个vring_used_elem的结构体，结构体的成员如下所示：
  - id      表示处理完成的IO request在vring_desc表中的组成的链表的头结点位置；
  - len    表示链表的长度。

如下图所示，vring_used表表示vring_desc表中的从0号位置开始的IO请求已经被递交给硬件，前端可以释放vring_desc表中的相应项。

![image-20220319164144093](/../virtio学习笔记.assets/image-20220319164144093.png)

**总结： 前端操作vring_desc vring_avail 表**

​             **后端从vring_desc中取请求，操作完成之后，更新vring_used表**

#### 3.3.2 对vring的操作

Vring的操作分为两部分：在前端虚拟机内，通过virtqueue_add_buf将IO请求的内存地址，放入vring_desc表中，同时更新vring_avail表；在后端QEMU进程内，根据vring_avail表的内容，通过virtqueue_get_buf从vring_desc表中取得数据，同时更新vring_used表。

##### **1) virtqueue_add_buf**

- no.1 将IO请求的地址存入当前空闲的vring_desc表中的addr（如果没有空闲表项，则通知后端完成读写请求，释放空间）；
- no.2 设置flags域，若本次IO请求还未完，则为VRING_DESC_F_NEXT，并转no.3；若本次IO请求的地址都已保存至vring_desc中，则为~VRING_DESC_F_NEXT，转no.4；
- no.3 根据next，找到下一个空闲的vrring_desc表项，跳转no.1;
- no.4 本次IO请求已全部存在vring_desc表中，并通过next域连接成了一个链表，将链表头结点在vring_desc表中位置写入vring_avail->ring[idx]，**并使idx++。(idx表示avail表中当前可用的索引)**

虚拟机内通过上述步骤将IO请求地址存至vring_desc表中，并通过kick函数通知前端来读取数据。

![image-20220319165346052](/../virtio学习笔记.assets/image-20220319165346052.png)

如上图所示，在add_buf之前vring_desc表中已经保存了一个IO请求链表，可以从vring_avail中知道，vring_desc表中的IO请求链表头结点位置为0，然后根据next遍历整个IO请求链表。

我们调用add_buf将本次IO请求放入vring_desc表中：在vring_desc表中的第三行添加一个数据项，flags域设置为NEXT,表示本次IO请求的内容还没有结束；从next域找到下一个空闲的vring_desc表项，即第4行，添加一行数据，flags域设置为~NEXT，表示本次IO请求的内容已经结束next域置为空。

更新vring_avail表，从idx找到viring_avali表中的第一个空闲位置（第2行），把添加到vring_desc表中的IO请求链表的头结点位置(也就是图中vring_desc表的第3行)，添加到vring_avail表中；更新vring_avail的idx加1。

疑问：填充vring_desc表时，是不是应该根据vring_avail表中的idx字段找到当前应该填充的位置？？？

##### **2) virtqueue_get_buf**

- 从vring_avail中取出数据，直到取到idx位置为止；
- 根据vring_avail中取到的值，从vring_desc中取出链表的头结点，并根据next域依次找到其余结点；
- 当IO请求被取出后，将链表头结点的位置值放入vring_used->ring[idx].id

![image-20220319170244262](/../virtio学习笔记.assets/image-20220319170244262.png)

如上图所示，在QEMU进行操作之前，vring_avial表中显示vring_desc表中有两个新的IO请求。

　　从vring_avail表中取出第一个IO请求的位置(vring_desc第0行)，从vring_desc表的第0行开始获取IO请求，若flags为NEXT则根据next继续往下寻找；若flags为~NEXT，则表示这个IO请求已经结束。QEMU将这个IO请求封装，发送硬件执行。

　　更新vring_used表，将从vring_desc取出的IO请求的链表的头结点位置存到vring_used->idx所指向的位置，并将idx加1。

　　这样当IO处理返回到虚拟机时，virtio驱动程序可以更具vring_uesd表中的信息释放vring_desc表的相应表项。

#### **3.3.3 前端对vring的管理**

​		vring属于vring_virtqueue，同时vring_vritqueue包含virtqueue。两部分分工明确：vring负责数据面，vritqueue负责控制面。

　　这部分以[virtio-balloon](https://www.cnblogs.com/edver/p/14684138.html)为例，分析前后端数据共享的源头：GUEST内部如何管理，既vring的诞生过程。

　　设备创建部分参考[virtio-balloon](https://www.cnblogs.com/edver/p/14684138.html)介绍，本节仅介绍virtio数据共享相关。

##### **3.3.3.1 结构体**

```c
/* virtio_balloon 驱动结构 */
struct virtio_balloon {
    struct virtio_device *vdev;
        /* balloon包含三个virtqueue */
    struct virtqueue *inflate_vq, *deflate_vq, *stats_vq;
        ...
}

struct virtqueue {
    struct list_head list;
    void (*callback)(struct virtqueue *vq);
    const char *name;
    struct virtio_device *vdev;
    unsigned int index;
    unsigned int num_free;
    void *priv;
};

struct vring_virtqueue {
    struct virtqueue vq;

    /* Actual memory layout for this queue */
    struct vring vring;
        ...
}
```

##### **3.3.3.2 数据共享区创建**

由linux驱动模型可知，驱动入口函数为virtballoon_probe，我们由此来分析数据共享区创建过程，整体调用逻辑如下：

　　**设备驱动层：**

```sh
virtballoon_probe ->
    init_vqs ->
        virtio_find_vqs ->
            vdev->config->find_vqs
```

　　**PCI设备层：**

```sh
(vdev->config->find_vqs)vp_modern_find_vqs ->
    vp_modern_find_vqs ->
        vp_find_vqs ->
            vp_find_vqs_intx/msix ->
                vp_setup_vq -> //实现pci设备中virtqueue的赋值
                    vp_dev->setup_vq  //真正创建virtqueue
```

**virtqueue创建：**

```c
setup_vq ->
    //1. 获取设备注册的virtqueue大小等信息
    vp_ioread16
    //2. 创建vring
    vring_create_virtqueue ->
        __vring_new_virtqueue
    //3. 共享内存地址通知qemu侧模拟的设备
    vp_iowrite16
    //4. 更新notify消息发送的地址
    vq->priv update
```



# virtio-balloon guest侧驱动

## 1、概述

​		在后端模拟出balloon设备后，guest os在启动时会扫描到此设备，遵循linux设备模型调用设备的初始化工作。Virtio-balloon属于 virtio体系，很多工作的细节需要再分析virtio的工作流程，本章暂且只分析balloon的行为，涉及virtio的部分插桩分析向后再补充分析。

balloon执行流程如下：

![image-20220319172634797](/../virtio学习笔记.assets/image-20220319172634797.png)

## 2、驱动创建

### 2.1 驱动注册

​	Linux设备驱动模型中，各驱动可以按总线类别进行划分，且每个总线类别下可以挂载“驱动”和“设备”两类对象。内核就维护了这样一张“总线”到“驱动和设备”的总表，每当一个新驱动加进内核时，内核会扫描该驱动所挂载总线上的所有设备，并通过**比对驱动中的id_table字段和设备配置空间中的Device ID**，如果相同则代表该驱动可以为该设备服务，那内核就会针对该设备**调用总线的probe函数**(**如果总线没有probe函数，再调用驱动的probe函数)。**

​		另外一种情况是往总线上插入一个新设备，内核同样会扫描总线上的所有驱动，**看哪个驱动匹配该设备，如果匹配也对该设备调用总线的probe函数(如果总线没有probe函数，再调用驱动的probe函数)**。

​		Linux内核中前端代码主要包括driver/virtio目录下相关文件，最终生成的内核模块有virtio.ko，virtio_ring.ko，virtio_pci.ko和virtio_balloon.ko。

​		由于virtio-balloon-pci设备是virtio-pci设备，而virtio-pci设备又是pci设备，所以virtio-pci设备的驱动会注册到pci总线上面，因此，整个初始化过程如下:

- 内核会首先找到virito-pci.ko这个驱动模块，并依次加载virtio.ko,virtio-ring.ko和virtio_pci.ko (virtio_pci.ko依赖前两个模块)执行其模块初始化函数，其中，virtio.ko模块会在系统中注册一种新的总线类型**virtio总线**，virtio_pci的初始化函数会调用其注册的virtio_pci_probe函数；
- virtio_pci_probe注册一个virtio设备(register_virtio_device)；
- 内核再次为这个virtio设备搜索驱动模块，最终找到virtio_balloon.ko并加载调用其模块初始化函数；
- virtio_balloon初始化函数在virtio总线上添加了virtio_balloon驱动并调用了总线的probe函数(总线的probe函数优先级高于总线上设备的probe函数)即virtio_dev_probe；
- virtio_dev_probe调用virtballoon_probe完成最后的初始化任务。

我们最终需要关注的是virtballoon_probe这个函数是怎么被调用到的，linux设备初始化开始到调用到virtballoon_probe的过程简化如下，仅供参考：

![image-20220319173155750](/../virtio学习笔记.assets/image-20220319173155750.png)

驱动可执行的动作包含在virtio_balloon_driver定义的结构体中。先来看下这个结构体的内容，文件位置driver/virtio/virtio_balloon.c。

```c
static unsigned int features[] = {
    VIRTIO_BALLOON_F_MUST_TELL_HOST,
    VIRTIO_BALLOON_F_STATS_VQ,
    VIRTIO_BALLOON_F_DEFLATE_ON_OOM,
};
 
static struct virtio_driver virtio_balloon_driver = {
    .feature_table = features,
    .feature_table_size = ARRAY_SIZE(features),
    .driver.name =  KBUILD_MODNAME,
    .driver.owner = THIS_MODULE,
    .id_table = id_table,
    .probe =    virtballoon_probe,
    .remove =   virtballoon_remove,
    .config_changed = virtballoon_changed,
#ifdef CONFIG_PM_SLEEP
    .freeze =   virtballoon_freeze,
    .restore =  virtballoon_restore,
#endif
};
module_virtio_driver(virtio_balloon_driver);
```

可以看到，注册的 driver中注册了feature属性，driver的名称和owner，驱动加载的probe卸载的remove，感知变化的config_changed，这三个函数做了主要的工作。 先来看下加载做了什么工作。

```c
static int virtballoon_probe(struct virtio_device *vdev)
{
    struct virtio_balloon *vb;
    int err;
 
    //device的get回调函数，用来获取qemu侧模拟的设备的config数据
    //回调在virtio_pci_modern.c中注册，原型为vp_get
    if (!vdev->config->get) {
        dev_err(&vdev->dev, "%s failure: config access disabled\n",
            __func__);
        return -EINVAL;
    }
    //申请一个virtio_balloon结构
    vdev->priv = vb = vb_dev = kmalloc(sizeof(*vb), GFP_KERNEL);
    if (!vb) {
        err = -ENOMEM;
        goto out;
    }
    //需要释放的页面默认为0，即gust默认保留全部页面，不使用balloon释放
    vb->num_pages = 0;
    mutex_init(&vb->balloon_lock);
    //初始化了两个工作队列，用于通知对应工作队列有消息到达，需要被唤醒
    init_waitqueue_head(&vb->config_change);
    init_waitqueue_head(&vb->acked);
    vb->vdev = vdev;
    vb->need_stats_update = 0;
    //尝试申请用于balloon的页面，如果失败一次则增加一
    //用来记录失败次数，如果短时间失败过多表明gust无多余内存可提供给balloon
    vb->alloc_page_tried = 0;
    //是否停止balloon，如gustos发生了lowmemkiller即内存不够gust使用，则停止balloon
    atomic_set(&vb->stop_balloon, 0);
 
    balloon_devinfo_init(&vb->vb_dev_info);
#ifdef CONFIG_BALLOON_COMPACTION
    vb->vb_dev_info.migratepage = virtballoon_migratepage;
#endif
    //初始化virtqueue，用于和后端设备进行通信
    //创建了3个queue用于ivq/dvq/svq时间的信息传输
    //同时注册了三个callback函数，用来唤醒上面写的两个工作队列
    err = init_vqs(vb);
    if (err)
        goto out_free_vb;
        //向oom的notify链表中添加处理回调函数，在out_of_memory函数中会调用
    vb->nb.notifier_call = virtballoon_oom_notify;
    vb->nb.priority = VIRTBALLOON_OOM_NOTIFY_PRIORITY;
    err = register_oom_notifier(&vb->nb);
    if (err < 0)
        goto out_oom_notify;
    //读取设备侧config的status，检查VIRTIO_CONFIG_S_DRIVER_OK是否置位
    //若已置位说明设备侧已经可用
    virtio_device_ready(vdev);
    //启动vballoon线程，balloon主要操作在这里完成
    vb->thread = kthread_run(balloon, vb, "vballoon");
    if (IS_ERR(vb->thread)) {
        err = PTR_ERR(vb->thread);
        goto out_del_vqs;
    }
 
    return 0;
 
out_del_vqs:
    unregister_oom_notifier(&vb->nb);
out_oom_notify:
    vdev->config->del_vqs(vdev);
out_free_vb:
    kfree(vb);
out:
    return err;
}
```

可以看到，这里的主要工作有：

- 通过init_waitqueue_head初始化了两个工作队列用来接收QEMU发来的notify
- 通过init_vqs初始化了3个 virt_queue用来和qemu发送balloon进行inflate/deflate的page地址信息以及callback回调
- 启动内核线程执行vballoon，执行balloon的具体操作

###  2.2 vballoon如何运作

```c
static int balloon(void *_vballoon)
{
    struct virtio_balloon *vb = _vballoon;
    //注册工作队列的唤醒函数
    DEFINE_WAIT_FUNC(wait, woken_wake_function);
 
    set_freezable();
    while (!kthread_should_stop()) {
        s64 diff;
 
        try_to_freeze();
        //将wait添加到config_change的队列，等待唤醒
        //唤醒操作需要virtballoon_changed处理，其注册到了驱动的config_changed
        //qemu执行virtio_notify_config发送notify时会被调用
        /*gust侧唤醒队列的调用栈如下
        vp_interrupt
          -> vp_config_changed
            -> virtio_config_changed
              -> __virtio_config_changed
                ->  drv->config_changed(virtballoon_changed)
        */
        add_wait_queue(&vb->config_change, &wait);
        for (;;) {
            //towards_target用来计算要释放的page数量->num_pages
            if (((diff = towards_target(vb)) != 0 &&
                vb->alloc_page_tried < 5) ||
                vb->need_stats_update ||
                !atomic_read(&vb->stop_balloon) ||
                kthread_should_stop() ||
                freezing(current))
                //需要执行balloon则退出这层循环
                break;
            wait_woken(&wait, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
             
            vb->alloc_page_tried = 0;
            atomic_set(&vb_dev->stop_balloon, 0);
        }
        //去除等待队列，处理时暂不接受新的balloon的notify
        remove_wait_queue(&vb->config_change, &wait);
        //更新stat信息，在初始化时置零，在stats_request调用时置一，并唤醒config_change队列
        //stats_request放入了virtqueue的callback
        if (vb->need_stats_update)
            stats_handle_request(vb);
        //diff大于零表示需要重gust申请内存放入balloon，释放内存
        //这样gust可用的内存减少，因为内存释放所以host可用内存增多
        if (diff > 0)
            fill_balloon(vb, diff);
        //diff小于零，表示gust需要从balloon中回收内存
        //这样gust可用内存增加，host内存被gust占用则可用内存减少
        else if (diff < 0)
            leak_balloon(vb, -diff);
        //更新balloon中记录的actual，刷新balloon实际申请到或释放掉的内存
        update_balloon_size(vb);
 
        /*
         * For large balloon changes, we could spend a lot of time
         * and always have work to do.  Be nice if preempt disabled.
         */
        cond_resched();
    }
    return 0;
}
```

主要涉及到的处理：

- 添加等待队列，等待config_change被唤醒，即QEMU有执行balloon操作
- 计算需要申请或者释放的空间，即diff值
- 如果需要申请或者释放空间，则调用fill_balloon或者leak_balloon进行操作
-  更新balloon实际占用的空间，记录到actual变量中，并通知给QEMU

​    计算diff值的操作如下

```c
static inline s64 towards_target(struct virtio_balloon *vb)
{
    s64 target;
    u32 num_pages;
    //获取最新的num_pages数据
    virtio_cread(vb->vdev, struct virtio_balloon_config, num_pages,
             &num_pages);
 
    /* Legacy balloon config space is LE, unlike all other devices. */
    if (!virtio_has_feature(vb->vdev, VIRTIO_F_VERSION_1))
        num_pages = le32_to_cpu((__force __le32)num_pages);
 
    target = num_pages;
    //使用最新的num_pages数据和已有的数据做差
    return target - vb->num_pages;
}
```

### 2.3 balloon充气过程

```c
static void fill_balloon(struct virtio_balloon *vb, size_t num)
{
    struct balloon_dev_info *vb_dev_info = &vb->vb_dev_info;
 
    /* We can only do one array worth at a time. */
    num = min(num, ARRAY_SIZE(vb->pfns));
 
    mutex_lock(&vb->balloon_lock);
    for (vb->num_pfns = 0; vb->num_pfns < num;
         vb->num_pfns += VIRTIO_BALLOON_PAGES_PER_PAGE) {
        //从gust空间申请一个页面，并且加入到vb_dev_info->pages链表中
        //并标记page的mapcount和设定private标志。这样可以让page不会被kernel继续使用
        struct page *page = balloon_page_enqueue(vb_dev_info);
 
        if (!page) {
            dev_info_ratelimited(&vb->vdev->dev,
                         "Out of puff! Can't get %u pages\n",
                         VIRTIO_BALLOON_PAGES_PER_PAGE);
            vb->alloc_page_tried++;
            /* Sleep for at least 1/5 of a second before retry. */
            msleep(200);
            break;
        }
        //清零页面申请失败计数
        vb->alloc_page_tried = 0;
        //填充vb->pfns数组对应项（不太清楚作用，需再分析）
        set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
        //num_pages为通知QEMU侧申请到的页面数量
        vb->num_pages += VIRTIO_BALLOON_PAGES_PER_PAGE;
        if (!virtio_has_feature(vb->vdev,
                    VIRTIO_BALLOON_F_DEFLATE_ON_OOM))
            adjust_managed_page_count(page, -1);
    }
 
    /* Did we get any? */
    if (vb->num_pfns != 0)
        //通过ivq队列将申请到的页面信息发送给qemu
        tell_host(vb, vb->inflate_vq);
    mutex_unlock(&vb->balloon_lock);
}
```

基本流程可以总结为：从gust空间申请页面放入balloon的链表中，并做标记使该内存内核不可用，填充设备的pfn数组，然后通过ivq通知设备侧进行处理。

###  2.4 leak_balloon过程

```c
static unsigned leak_balloon(struct virtio_balloon *vb, size_t num)
{
    unsigned num_freed_pages;
    struct page *page;
    struct balloon_dev_info *vb_dev_info = &vb->vb_dev_info;
 
    /* We can only do one array worth at a time. */
    num = min(num, ARRAY_SIZE(vb->pfns));
 
    mutex_lock(&vb->balloon_lock);
    /* We can't release more pages than taken */
    num = min(num, (size_t)vb->num_pages);
    for (vb->num_pfns = 0; vb->num_pfns < num;
         vb->num_pfns += VIRTIO_BALLOON_PAGES_PER_PAGE) {
        //将申请到balloon的页面释放出来
        page = balloon_page_dequeue(vb_dev_info);
        if (!page)
            break;
        //设置pfn数组
        set_page_pfns(vb, vb->pfns + vb->num_pfns, page);
        vb->num_pages -= VIRTIO_BALLOON_PAGES_PER_PAGE;
    }
 
    num_freed_pages = vb->num_pfns;
    /*
     * Note that if
     * virtio_has_feature(vdev, VIRTIO_BALLOON_F_MUST_TELL_HOST);
     * is true, we *have* to do it in this order
     */
    if (vb->num_pfns != 0)
        //使用dvq通知qemu进行处理
        tell_host(vb, vb->deflate_vq);
    release_pages_balloon(vb);
    mutex_unlock(&vb->balloon_lock);
    return num_freed_pages;
}
```

leak_balloon的过程和fill_balloon刚好相反，它会释放存放在balloon的page链表中的page项归还给guest，同理，这部分内存会被qemu从host申请回来留给guest os备用，此时host主机的可用内存就减少了。

# virtio-balloon qemu设备创建

## 1、概述

根据前一章信息，virtio设备分为前端设备/通信层/后端设备，本章从后端设备设备（qemu的balloon设备为例）的初始化开始分析。

从启动到balloon设备开始初始化基本调用流程如下：

![img](virtio学习笔记.assets\774036-20210421101624613-1715754909.png)

balloon代码执行流程如下：

![img](virtio学习笔记.assets\774036-20210421101645830-1515162619.png)

## 2、关键数据结构

### 2.1 balloon设备结构

```c
typedef struct VirtIOBalloon {
    VirtIODevice parent_obj;
    VirtQueue *ivq, *dvq, *svq;  // 3个 virt queue
    // pages we want guest to give up 
    uint32_t num_pages; 
    // pages in balloon
    uint32_t actual;
    uint64_t stats[VIRTIO_BALLOON_S_NR];  // status 
     
    // status virtqueue 会用到
    VirtQueueElement *stats_vq_elem;
    size_t stats_vq_offset;
     
    // 定时器, 定时查询功能
    QEMUTimer *stats_timer;
    int64_t stats_last_update;
    int64_t stats_poll_interval;
     
    // features
    uint32_t host_features;
    // for adjustmem, reserved guest free memory
    uint64_t res_size;
} VirtIOBalloon;
```

分析：

- num_pages字段是balloon中表示我们希望guest归还给host的内存大小
- actual字段表示balloon实际捕获的pages数目

guest处理configuration change中断，完成之后调用virtio_cwrite函数。因为写balloon设备的配置空间，所以陷出，qemu收到后会找到balloon设备，修改config修改config时，更新balloon->actual字段

- stats_last_update在每次从status virtioqueue中取出数据时更新

### 2.2 消息通讯结构VirtQueue

```c
struct VirtQueue
{
    VRing vring;
 
    /* Next head to pop */
    uint16_t last_avail_idx;
 
    /* Last avail_idx read from VQ. */
    uint16_t shadow_avail_idx;
 
    uint16_t used_idx;
 
    /* Last used index value we have signalled on */
    uint16_t signalled_used;
 
    /* Last used index value we have signalled on */
    bool signalled_used_valid;
 
    /* Notification enabled? */
    bool notification;
 
    uint16_t queue_index;
    //队列中正在处理的请求的数目
    unsigned int inuse;
 
    uint16_t vector;
    //回调函数
    VirtIOHandleOutput handle_output;
    VirtIOHandleAIOOutput handle_aio_output;
    VirtIODevice *vdev;
    EventNotifier guest_notifier;
    EventNotifier host_notifier;
    QLIST_ENTRY(VirtQueue) node;
};
```

## 3. 初始化流程

### 3.1 设备类型注册

```c
type_init(virtio_register_types)
    type_register_static(&virtio_balloon_info);
        ->instance_init = virtio_balloon_instance_init,
        ->class_init = virtio_balloon_class_init,
```

### 3.2 类及实例初始化 

```c
qemu_opts_foreach(qemu_find_opts("device"), device_init_func, NULL, NULL)   //vl.c
  qdev_device_add                               //qdev-monitor.c
    object_new()                
       ->class_init
       ->instance_init
    object_property_set_bool(realized)  --> virtio_balloon_device_realize    //virtio-balloon.c
       ->virtio_init
       ->virtio_add_queue
```

###  3.3 balloon设备实例化

virtio_balloon_device_realize实例化函数主要执行两个函数完成实例化操作，首先调用virtio_init初始化virtio设备的公共部分。 virtio_init的 主要工作是初始化所有virtio设备的基类TYPE_VIRTIO_DEVICE（"virtio-device"）的实例VirtIODevice结构体。

实例化代码简化实现如下：

```c
static void virtio_balloon_device_realize(DeviceState *dev, Error **errp)
{
    virtio_init(vdev, "virtio-balloon", VIRTIO_ID_BALLOON,
                sizeof(struct virtio_balloon_config));
 
    ret = qemu_add_balloon_handler(virtio_balloon_to_target,
                                   virtio_balloon_stat,
                                   virtio_balloon_adjustmem,
                                   virtio_balloon_get_stats, s);
 
...
 
    s->ivq = virtio_add_queue(vdev, 128, virtio_balloon_handle_output);
    s->dvq = virtio_add_queue(vdev, 128, virtio_balloon_handle_output);
    s->svq = virtio_add_queue(vdev, 128, virtio_balloon_receive_stats);
 
    reset_stats(s);
}
```

 virio_init的代码流程和基本成员注释如下：

```c
void virtio_init(VirtIODevice *vdev, const char *name,
                 uint16_t device_id, size_t config_size)
{
    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));
    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);
    int i;
    int nvectors = k->query_nvectors ? k->query_nvectors(qbus->parent) : 0;
 
    if (nvectors) {
        //vector_queues与 MSI中断相关
        vdev->vector_queues =
            g_malloc0(sizeof(*vdev->vector_queues) * nvectors);
    }
 
    vdev->device_id = device_id;
    vdev->status = 0;
    atomic_set(&vdev->isr, 0);  //中断请求
    vdev->queue_sel = 0;    //配置队列的时候选择队列
    //config_vector与MSI中断相关
    vdev->config_vector = VIRTIO_NO_VECTOR;
    //vq分配了1024个virtQueue并进行初始化
    vdev->vq = g_malloc0(sizeof(VirtQueue) * VIRTIO_QUEUE_MAX);
    vdev->vm_running = runstate_is_running();
    vdev->broken = false;
    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
        vdev->vq[i].vector = VIRTIO_NO_VECTOR;
        vdev->vq[i].vdev = vdev;
        vdev->vq[i].queue_index = i;
    }
 
    vdev->name = name;
    //config_len表示配置空间的长度
    vdev->config_len = config_size;
    if (vdev->config_len) {
        //config表示配置数据的存放区域
        vdev->config = g_malloc0(config_size);
    } else {
        vdev->config = NULL;
    }
    vdev->vmstate = qemu_add_vm_change_state_handler(virtio_vmstate_change,
                                                     vdev);
    vdev->device_endian = virtio_default_endian();
    //use_guest_notifier_mask与irqfd有关
    vdev->use_guest_notifier_mask = true;
}
```

   virtio_init主要操作为：

- 设置中断
- 申请virtqueue空间
- 申请配置数据空间

初始化操作完成后，realize函数继续调用virtio_add_queue创建了3个virtqueue（ivq、dvq、svq）并将回调函数virtio_balloon_handle_output挂接到virtqueue的handle_output，用于处理virtqueue中的数据，handle_output函数处理在消息通信一节再分析。 virtio_add_queue实现如下:

```c
VirtQueue *virtio_add_queue(VirtIODevice *vdev, int queue_size,
                            VirtIOHandleOutput handle_output)
{
    int i;
 
    for (i = 0; i < VIRTIO_QUEUE_MAX; i++) {
        if (vdev->vq[i].vring.num == 0)
            break;
    }
 
    if (i == VIRTIO_QUEUE_MAX || queue_size > VIRTQUEUE_MAX_SIZE)
        abort();
 
    vdev->vq[i].vring.num = queue_size;
    vdev->vq[i].vring.num_default = queue_size;
    vdev->vq[i].vring.align = VIRTIO_PCI_VRING_ALIGN;
    vdev->vq[i].handle_output = handle_output;
    vdev->vq[i].handle_aio_output = NULL;
 
    return &vdev->vq[i];
}
```

## 4. balloon处理

### 4.1 回调函数处理流程

上一章分析到realize函数注册了3个virtqueue的回调函数，先分析inflate和deflate（ivq和dvq）涉及的函数，查询状态信息的函数稍后分析。ivq和dvq注册的handle_output为virtio_balloon_handle_output，当gust侧通过virtqueue进行通知的时候会调用handle_out对数据进行处理。

```c
static void virtio_balloon_handle_output(VirtIODevice *vdev, VirtQueue *vq)
{
    VirtIOBalloon *s = VIRTIO_BALLOON(vdev);
    VirtQueueElement *elem;
    MemoryRegionSection section;
 
    for (;;) {
        size_t offset = 0;
        uint32_t pfn;
        //获取virtqueue中的数据到qemu侧virt-ring通用的数据结构
        //handle_out函数通用操作
        elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
        if (!elem) {
            if (hax_enabled() && vq == s->dvq) {
                hax_issue_invept();
            }
            return;
        }
 
        while (iov_to_buf(elem->out_sg, elem->out_num, offset, &pfn, 4) == 4) {
            ram_addr_t pa;
            ram_addr_t addr;
            int p = virtio_ldl_p(vdev, &pfn);
            //将页框转换成GPA
            pa = (ram_addr_t) p << VIRTIO_BALLOON_PFN_SHIFT;
            offset += 4;
 
            //根据pa找到对应的MemoryRegionSection
            section = memory_region_find(get_system_memory(), pa, 1);
            if (!int128_nz(section.size) ||
                !memory_region_is_ram(section.mr) ||
                memory_region_is_rom(section.mr) ||
                memory_region_is_romd(section.mr)) {
                trace_virtio_balloon_bad_addr(pa);
                memory_region_unref(section.mr);
                continue;
            }
 
            trace_virtio_balloon_handle_output(memory_region_name(section.mr),
                                               pa);
            /* Using memory_region_get_ram_ptr is bending the rules a bit, but
               should be OK because we only want a single page.  */
            addr = section.offset_within_region;
            //根据section获取对应的HVA，然后调用balloon函数处理对应页面
            balloon_page(memory_region_get_ram_ptr(section.mr) + addr, pa,
                         !!(vq == s->dvq));
            memory_region_unref(section.mr);
        }
 
        //处理完后通知gust，此处为handle_out通用操作
        virtqueue_push(vq, elem, offset);
        virtio_notify(vdev, vq);
        g_free(elem);
    }
}
```

handle_output函数使用virtqueue_pop取出virtqueue中对应的数据到VirtQueueElement结构体中，在经过地址转换后得到了HVA地址，然后将HVA和队列信息（dvq/ivq?）传入balloon_page进行qemu侧的balloon处理。

### 4.2 qemu处理队列分类

​    balloon_page根据deflate参数判断此次操作时inflate还是deflate，分如下操作：

- 如果使deflate操作，直接返回。因为deflate操作表示gust会再次使用对应的页面地址，主要是gust内部取消掉这部分页面不可用的标志，QEMU侧因为提供给gust的虚拟地址空间一直是保留状态所以无需特殊处理

-  如果使inflate操作，表示对应的页面将不会再提供给gust使用，所以此时先取消对应的ept映射再对QEMU侧的HVA地址使用qemu_madvise进行处理。

​    具体代码如下：

```c
static void balloon_page(void *addr, ram_addr_t gpa, int deflate)
{
    if (!qemu_balloon_is_inhibited() && (!kvm_enabled() ||
                                         kvm_has_sync_mmu())) {
#ifdef _WIN32
        if (!hax_enabled() || !hax_ept_set_supported()) {
            return;
        }
        // For deflation, ept entry can be rebuilt via VMX EPT VIOLATION.
        if (deflate || hax_invalid_ept_entries(gpa, BALLOON_PAGE_SIZE)) {
            return;
        }
#endif
 
        qemu_madvise(addr, BALLOON_PAGE_SIZE,
                deflate ? QEMU_MADV_WILLNEED : QEMU_MADV_DONTNEED);
    }
}
```

4.3 qemu处理虚拟内存

​     balloon_page对操作类型分类后，调用qemu_madvise针对不同操作系统处理虚拟地址空间，windows上流程如下：

qemu_madvise-> win32_madvise。

win32_madvise处理两种情况willneed和dontneed，分别表示deflate和inflate过程，上一步已经说明过deflate过程主要在GUST侧取消页面不可用标记，这里目前只处理dontneed过程。

 因为windows中虚拟地址申请函数VirtualAlloc可以有提交(commit)和保留（reserve）操作，只有commit的页面才可以在访问时申请物理空间。

在系统初始化时（参考[这里](http://http//3ms.huawei.com/km/blogs/details/9732599?l=zh-cn)的pc.ram的初始化流程），qemu_anon_ram_alloc函数使用VirtualAlloc（MEM_COMMIT | MEM_RESERVE）为pc.ram保留并提交了4G空间（可配置，不一定是4G)。所以GUST访问的空间都是已经提交过并且保留下来不会被其他malloc之类的函数占用的，因此这4G是连续的。

当guest执行inflate操作后，放入balloon中的页面也不会再被访问，在上一步中取消EPT映射后需要在free掉对应的虚拟地址以释放内存，但是为了保证pc.ram的内存连续并且随时可用，所以free后再次virtualAlloc（MEM_COMMIT），保持页面是提交状态，避免gust进行deflate后访问对应界面而发生异常。对应代码如下:

```c
int win32_madvise(void *addr, size_t len, int advice)
{
    const size_t page_size = qemu_real_host_page_size;
    LPVOID start = (LPVOID)QEMU_ALIGN_PTR_DOWN(addr, page_size);
    len = ROUND_UP(len, page_size);
 
    switch (advice) {
    case QEMU_MADV_WILLNEED:
        return 0;
    case QEMU_MADV_DONTNEED: {
        /*
         * We have not chosen DiscardVirtualMemory() due to its low performance.
         * Besides, we dont have to call VirtualUnlock here, because free will call unlock internally.
         */
        if (!VirtualFree(start, len, MEM_DECOMMIT)) {
            fprintf(stderr, "%s failed to decommmit memory: %lu\n",
                    __func__, GetLastError());
            break;
        }
        if (!VirtualAlloc(start, len, MEM_COMMIT, PAGE_READWRITE)) {
            fprintf(stderr, "%s failed to re-commit memory: %lu\n",
                    __func__, GetLastError());
            break;
        }
 
        return 0;
    }
    }
 
    return -EINVAL;
}
```



# 从零实现一个virtio设备

## 1、概述

前几节分析了virtio机制和现有的balloon设备实现，至此我们已经知道了virtio是什么、怎么使用的，本节我们就自己实现一个virtio纯虚设备。

功能：

- QEMU模拟的设备启动一个定时器，每5秒发送一次中断通知GUEST
- GUEST对应的驱动接收到中断将自身变量自增，然后通过vring发送给QEMU
- QEMU收到GUEST发送过来的消息后打印出接收到的数值 

## 2、设备创建

### 2.1 添加virtio id

用于guest内部的设备和驱动的match，需要和linux内核中定义的一致。

文件：include/standard-headers/linux/virtio_ids.h

```c
#define VIRTIO_ID_TEST       39 /* virtio test */
```

### 2.2 添加device id

vendor-id和device-id用于区分PCI设备,**注意不要超过0x104f**

文件: include/hw/pci/pci.h

```c
#define PCI_DEVICE_ID_VIRTIO_TEST       0x1013
```

### 2.3  添加virtio-test设备配置空间定义的头文件

定义与GUEST协商配置的feature和config结构体，需要与linux中定义一致，config在本示例中并未使用，结构拷贝自balloon

文件： include/standard-headers/linux/virtio_test.h

```c
#ifndef _LINUX_VIRTIO_TEST_H
#define _LINUX_VIRTIO_TEST_H

#include "standard-headers/linux/types.h"
#include "standard-headers/linux/virtio_types.h"
#include "standard-headers/linux/virtio_ids.h"
#include "standard-headers/linux/virtio_config.h"

#define VIRTIO_TEST_F_CAN_PRINT    0

struct virtio_test_config {
    /* Number of pages host wants Guest to give up. */
    uint32_t num_pages;
    /* Number of pages we've actually got in balloon. */
    uint32_t actual;
    /* Event host wants Guest to do */
    uint32_t event;
};

struct virtio_test_stat {
    __virtio16 tag;
    __virtio64 val;
} QEMU_PACKED;

#endif
```

### 2.4 添加virtio-test设备模拟代码

此代码包括了对vring的操作和简介中的功能主体实现，与驱动交互的代码逻辑都在这里。

文件：hw/virtio/virtio-test.c

```c
#include "qemu/osdep.h"
#include "qemu/log.h"
#include "qemu/iov.h"
#include "qemu/timer.h"
#include "qemu-common.h"
#include "hw/virtio/virtio.h"
#include "hw/virtio/virtio-test.h"
#include "sysemu/kvm.h"
#include "sysemu/hax.h"
#include "exec/address-spaces.h"
#include "qapi/error.h"
#include "qapi/qapi-events-misc.h"
#include "qapi/visitor.h"
#include "qemu/error-report.h"

#include "hw/virtio/virtio-bus.h"
#include "hw/virtio/virtio-access.h"
#include "migration/migration.h"


static void virtio_test_handle_output(VirtIODevice *vdev, VirtQueue *vq)
{
    VirtIOTest *s = VIRTIO_TEST(vdev);
    VirtQueueElement *elem;
    MemoryRegionSection section;

    for (;;) {
        size_t offset = 0;
        uint32_t pfn;
        elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
        if (!elem) {
            return;
        }

        while (iov_to_buf(elem->out_sg, elem->out_num, offset, &pfn, 4) == 4) {
            int p = virtio_ldl_p(vdev, &pfn);

            offset += 4;
            qemu_log("=========get virtio num:%d\n", p);
        }

        virtqueue_push(vq, elem, offset);
        virtio_notify(vdev, vq);
        g_free(elem);
    }
}


static void virtio_test_get_config(VirtIODevice *vdev, uint8_t *config_data)
{
    VirtIOTest *dev = VIRTIO_TEST(vdev);
    struct virtio_test_config config;

    config.actual = cpu_to_le32(dev->actual);
    config.event = cpu_to_le32(dev->event);

    memcpy(config_data, &config, sizeof(struct virtio_test_config));

}

static void virtio_test_set_config(VirtIODevice *vdev,
                                      const uint8_t *config_data)
{
    VirtIOTest *dev = VIRTIO_TEST(vdev);
    struct virtio_test_config config;

    memcpy(&config, config_data, sizeof(struct virtio_test_config));
    dev->actual = le32_to_cpu(config.actual);
    dev->event = le32_to_cpu(config.event);
}

static uint64_t virtio_test_get_features(VirtIODevice *vdev, uint64_t f,
                                            Error **errp)
{
    VirtIOTest *dev = VIRTIO_TEST(vdev);
    f |= dev->host_features;
    virtio_add_feature(&f, VIRTIO_TEST_F_CAN_PRINT);

    return f;
}

static int virtio_test_post_load_device(void *opaque, int version_id)
{
    VirtIOTest *s = VIRTIO_TEST(opaque);

    return 0;
}

static const VMStateDescription vmstate_virtio_test_device = {
    .name = "virtio-test-device",
    .version_id = 1,
    .minimum_version_id = 1,
    .post_load = virtio_test_post_load_device,
    .fields = (VMStateField[]) {
        VMSTATE_UINT32(actual, VirtIOTest),
        VMSTATE_END_OF_LIST()
    },
};

static void test_stats_change_timer(VirtIOTest *s, int64_t secs)
{
    timer_mod(s->stats_timer, qemu_clock_get_ms(QEMU_CLOCK_VIRTUAL) + secs * 1000);
}

static void test_stats_poll_cb(void *opaque)
{
    VirtIOTest *s = opaque;
    VirtIODevice *vdev = VIRTIO_DEVICE(s);

    qemu_log("==============set config:%d\n", s->set_config++);
    virtio_notify_config(vdev);
    test_stats_change_timer(s, 1);
}

static void virtio_test_device_realize(DeviceState *dev, Error **errp)
{
    VirtIODevice *vdev = VIRTIO_DEVICE(dev);
    VirtIOTest *s = VIRTIO_TEST(dev);
    int ret;

    virtio_init(vdev, "virtio-test", VIRTIO_ID_TEST,
                sizeof(struct virtio_test_config));

    s->ivq = virtio_add_queue(vdev, 128, virtio_test_handle_output);

    /* create a new timer */
    g_assert(s->stats_timer == NULL);
    s->stats_timer = timer_new_ms(QEMU_CLOCK_VIRTUAL, test_stats_poll_cb, s);
    test_stats_change_timer(s, 30);
}

static void virtio_test_device_unrealize(DeviceState *dev, Error **errp)
{
    VirtIODevice *vdev = VIRTIO_DEVICE(dev);
    VirtIOTest *s = VIRTIO_TEST(dev);

    virtio_cleanup(vdev);
}

static void virtio_test_device_reset(VirtIODevice *vdev)
{
    VirtIOTest *s = VIRTIO_TEST(vdev);
}

static void virtio_test_set_status(VirtIODevice *vdev, uint8_t status)
{
    VirtIOTest *s = VIRTIO_TEST(vdev);
    return;
}

static void virtio_test_instance_init(Object *obj)
{
    VirtIOTest *s = VIRTIO_TEST(obj);

    return;
}

static const VMStateDescription vmstate_virtio_test = {
    .name = "virtio-test",
    .minimum_version_id = 1,
    .version_id = 1,
    .fields = (VMStateField[]) {
        VMSTATE_VIRTIO_DEVICE,
        VMSTATE_END_OF_LIST()
    },
};

static Property virtio_test_properties[] = {
    DEFINE_PROP_END_OF_LIST(),
};

static void virtio_test_class_init(ObjectClass *klass, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(klass);
    VirtioDeviceClass *vdc = VIRTIO_DEVICE_CLASS(klass);

    dc->props = virtio_test_properties;
    dc->vmsd = &vmstate_virtio_test;
    set_bit(DEVICE_CATEGORY_MISC, dc->categories);
    vdc->realize = virtio_test_device_realize;
    vdc->unrealize = virtio_test_device_unrealize;
    vdc->reset = virtio_test_device_reset;
    vdc->get_config = virtio_test_get_config;
    vdc->set_config = virtio_test_set_config;
    vdc->get_features = virtio_test_get_features;
    vdc->set_status = virtio_test_set_status;
    vdc->vmsd = &vmstate_virtio_test_device;
}

static const TypeInfo virtio_test_info = {
    .name = TYPE_VIRTIO_TEST,
    .parent = TYPE_VIRTIO_DEVICE,
    .instance_size = sizeof(VirtIOTest),
    .instance_init = virtio_test_instance_init,
    .class_init = virtio_test_class_init,
};

static void virtio_register_types(void)
{
    type_register_static(&virtio_test_info);
}

type_init(virtio_register_types)
```

文件： include/hw/virtio/virtio-test.h

```c
#ifndef QEMU_VIRTIO_TEST_H
#define QEMU_VIRTIO_TEST_H

#include "standard-headers/linux/virtio_test.h"
#include "hw/virtio/virtio.h"
#include "hw/pci/pci.h"

#define TYPE_VIRTIO_TEST "virtio-test-device"
#define VIRTIO_TEST(obj) \
        OBJECT_CHECK(VirtIOTest, (obj), TYPE_VIRTIO_TEST)


typedef struct VirtIOTest {
    VirtIODevice parent_obj;
    VirtQueue *ivq;
    uint32_t set_config;
    uint32_t actual;
    VirtQueueElement *stats_vq_elem;
    size_t stats_vq_offset;
    QEMUTimer *stats_timer;
    uint32_t host_features;
    uint32_t event;
} VirtIOTest;

#endif
```

### 2.5 virtio-test-pci设备实现

virtio-test设备属于virtio设备挂接在virtio总线上，但是virtio属于PCI设备。真正的设备发现和配置操作都依赖于PCI协议，因此将virtio-test设备包含于virtio-test-pci中，提供给外层的感知是这是一个pci设备，遵循PCI协议的规范。

头文件： hw/virtio/virtio-pci.h

```c
 #include "hw/virtio/virtio-gpu.h"
 #include "hw/virtio/virtio-crypto.h"
 #include "hw/virtio/vhost-user-scsi.h"
+#include "hw/virtio/virtio-test.h"
 #if defined(CONFIG_VHOST_USER) && defined(CONFIG_LINUX)
 #include "hw/virtio/vhost-user-blk.h"
 #endif typedef struct VirtIOGPUPCI VirtIOGPUPCI;
 typedef struct VHostVSockPCI VHostVSockPCI;
 typedef struct VirtIOCryptoPCI VirtIOCryptoPCI;
 typedef struct VirtIOWifiPCI VirtIOWifiPCI;
+typedef struct VirtIOTestPCI VirtIOTestPCI;+/*
+ * virtio-test-pci: This extends VirtioPCIProxy.
+ */
+#define TYPE_VIRTIO_TEST_PCI "virtio-test-pci"
+#define VIRTIO_TEST_PCI(obj) \
+        OBJECT_CHECK(VirtIOTestPCI, (obj), TYPE_VIRTIO_TEST_PCI)
+
+struct VirtIOTestPCI {
+    VirtIOPCIProxy parent_obj;
+    VirtIOTest vdev;
+};
```

文件： hw/virtio/virtio-pci.c

```c
/* virtio-test-pci */
static Property virtio_test_pci_properties[] = {
    DEFINE_PROP_UINT32("class", VirtIOPCIProxy, class_code, 0),
    DEFINE_PROP_END_OF_LIST(),
};

static void virtio_test_pci_realize(VirtIOPCIProxy *vpci_dev, Error **errp)
{
    VirtIOTestPCI *dev = VIRTIO_TEST_PCI(vpci_dev);
    DeviceState *vdev = DEVICE(&dev->vdev);

    if (vpci_dev->class_code != PCI_CLASS_OTHERS &&
        vpci_dev->class_code != PCI_CLASS_MEMORY_RAM) { /* qemu < 1.1 */
        vpci_dev->class_code = PCI_CLASS_OTHERS;
    }

    qdev_set_parent_bus(vdev, BUS(&vpci_dev->bus));
    object_property_set_bool(OBJECT(vdev), true, "realized", errp);
}

static void virtio_test_pci_class_init(ObjectClass *klass, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(klass);
    VirtioPCIClass *k = VIRTIO_PCI_CLASS(klass);
    PCIDeviceClass *pcidev_k = PCI_DEVICE_CLASS(klass);
    k->realize = virtio_test_pci_realize;
    set_bit(DEVICE_CATEGORY_MISC, dc->categories);
    dc->props = virtio_test_pci_properties;
    pcidev_k->vendor_id = PCI_VENDOR_ID_REDHAT_QUMRANET;
    pcidev_k->device_id = PCI_DEVICE_ID_VIRTIO_TEST;
    pcidev_k->revision = VIRTIO_PCI_ABI_VERSION;
    pcidev_k->class_id = PCI_CLASS_OTHERS;
}

static void virtio_test_pci_instance_init(Object *obj)
{
    VirtIOTestPCI *dev = VIRTIO_TEST_PCI(obj);

    virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
                                TYPE_VIRTIO_TEST);
}

static const TypeInfo virtio_test_pci_info = {
    .name          = TYPE_VIRTIO_TEST_PCI,
    .parent        = TYPE_VIRTIO_PCI,
    .instance_size = sizeof(VirtIOTestPCI),
    .instance_init = virtio_test_pci_instance_init,
    .class_init    = virtio_test_pci_class_init,
};

@@ -2739,6 +2789,7 @@ static void virtio_pci_register_types(void)
     type_register_static(&virtio_scsi_pci_info);
     type_register_static(&virtio_balloon_pci_info);
+    type_register_static(&virtio_test_pci_info);
     type_register_static(&virtio_serial_pci_info);
     type_register_static(&virtio_net_pci_info);
```

### 2.6 使设备生效

- 上述代码没有添加将virtio-test.c加入编译工程的代码，需要在对应CMake工程中将C文件加入，设置include目录（-I）的地方不要漏掉
- 完成后编译生成可执行文件
- 执行启动命令时加入对应参数： -qemu -device virtio-test-pci
- 在hmp界面输入info qtree可以看到设备已经创建

![img](virtio学习笔记.assets\774036-20220209104652708-966561027.png)

- 进入guest找到 /sys/buc/pci/devices目录，这里的第19就是我们新建的设备

![img](virtio学习笔记.assets\774036-20220210094813131-1908199180.png)

## 3、GUEST内驱动实现

### 3.1 添加virtio id

需要和设备定义的virtio id一致，用于设备和驱动的match

文件：include/uapi/linux/virtio_ids.h

```c
#define VIRTIO_ID_TEST       21 /* virtio test */
```

### 3.2 添加virtio-test驱动配置空间结构定义头文件

文件内容和QEMU定义相同，用于驱动和设备协商配置和feature

文件：include/uapi/linux/virtio_test.h

```c
#ifndef _LINUX_VIRTIO_TEST_H
#define _LINUX_VIRTIO_TEST_H
#include <linux/types.h>
#include <linux/virtio_types.h>
#include <linux/virtio_ids.h>
#include <linux/virtio_config.h>

/* The feature bitmap for virtio balloon */
#define VIRTIO_TEST_F_CAN_PRINT 0


struct virtio_test_config {
    /* Number of pages host wants Guest to give up. */
    __u32 num_pages;
    /* Number of pages we've actually got in balloon. */
    __u32 actual;
};

struct virtio_test_stat {
    __virtio16 tag;
    __virtio64 val;
} __attribute__((packed));

#endif /* _LINUX_VIRTIO_TEST_H */
```

### 3.3 添加virtio-test驱动实现

```c
#include <linux/virtio.h>
#include <linux/virtio_test.h>
#include <linux/swap.h>
#include <linux/workqueue.h>
#include <linux/delay.h>
#include <linux/slab.h>
#include <linux/module.h>
#include <linux/oom.h>
#include <linux/wait.h>
#include <linux/mm.h>
#include <linux/mount.h>
#include <linux/magic.h>


struct virtio_test {
    struct virtio_device *vdev;
    struct virtqueue *print_vq;

    struct work_struct print_val_work;
    bool stop_update;
    atomic_t stop_once;

    /* Waiting for host to ack the pages we released. */
    wait_queue_head_t acked;

    __virtio32 num[256];
};

static struct virtio_device_id id_table[] = {
    { VIRTIO_ID_TEST, VIRTIO_DEV_ANY_ID },
    { 0 },
};

static struct virtio_test *vb_dev;

static void test_ack(struct virtqueue *vq)
{
    struct virtio_test *vb = vq->vdev->priv;
    printk("virttest get ack\n");
    unsigned int len;
    virtqueue_get_buf(vq, &len);
}

static int init_vqs(struct virtio_test *vb)
{
    struct virtqueue *vqs[1];
    vq_callback_t *callbacks[] = { test_ack };
    static const char * const names[] = { "print"};
    int err, nvqs;

    nvqs = virtio_has_feature(vb->vdev, VIRTIO_TEST_F_CAN_PRINT) ? 1 : 0;
    err = virtio_find_vqs(vb->vdev, nvqs, vqs, callbacks, names, NULL);
    if (err)
        return err;

    vb->print_vq = vqs[0];

    return 0;
}

static void remove_common(struct virtio_test *vb)
{
    /* Now we reset the device so we can clean up the queues. */
    vb->vdev->config->reset(vb->vdev);

    vb->vdev->config->del_vqs(vb->vdev);
}

static void virttest_remove(struct virtio_device *vdev)
{
    struct virtio_test *vb = vdev->priv;

    remove_common(vb);
    cancel_work_sync(&vb->print_val_work);
    kfree(vb);
    vb_dev = NULL;
}

static int virttest_validate(struct virtio_device *vdev)
{
    return 0;
}

static void print_val_func(struct work_struct *work)
{
    struct virtio_test *vb;
    struct scatterlist sg;

    vb = container_of(work, struct virtio_test, print_val_work);
    printk("virttest get config change\n");

    struct virtqueue *vq = vb->print_vq;
    vb->num[0]++;
    sg_init_one(&sg, &vb->num[0], sizeof(vb->num[0]));

    /* We should always be able to add one buffer to an empty queue. */
    virtqueue_add_outbuf(vq, &sg, 1, vb, GFP_KERNEL);
    virtqueue_kick(vq);
}

static void virttest_changed(struct virtio_device *vdev)
{
    struct virtio_test *vb = vdev->priv;
    printk("virttest virttest_changed\n");
    if (!vb->stop_update) {
        //atomic_set(&vb->stop_once, 0);
        queue_work(system_freezable_wq, &vb->print_val_work);
    }
}

static int virttest_probe(struct virtio_device *vdev)
{
    struct virtio_test *vb;
    int err;

    printk("******create virttest\n");
    if (!vdev->config->get) {
        return -EINVAL;
    }

    vdev->priv = vb = kmalloc(sizeof(*vb), GFP_KERNEL);
    if (!vb) {
        err = -ENOMEM;
        goto out;
    }
    vb->num[0] = 0;
    vb->vdev = vdev;
    INIT_WORK(&vb->print_val_work, print_val_func);

    vb->stop_update = false;

    init_waitqueue_head(&vb->acked);
    err = init_vqs(vb);
    if (err)
        goto out_free_vb;

    virtio_device_ready(vdev);

    atomic_set(&vb->stop_once, 0);
    vb_dev = vb;

    return 0;

out_free_vb:
    kfree(vb);
out:
    return err;
}

static unsigned int features[] = {
    VIRTIO_TEST_F_CAN_PRINT,
};

static struct virtio_driver virtio_test_driver = {
    .feature_table = features,
    .feature_table_size = ARRAY_SIZE(features),
    .driver.name =  KBUILD_MODNAME,
    .driver.owner = THIS_MODULE,
    .id_table = id_table,
    .validate = virttest_validate,
    .probe =    virttest_probe,
    .remove =   virttest_remove,
    .config_changed = virttest_changed,
};

module_virtio_driver(virtio_test_driver);
MODULE_DEVICE_TABLE(virtio, id_table);
MODULE_DESCRIPTION("Virtio test driver");
MODULE_LICENSE("GPL");
```

### 3.4 新驱动编译进内核

为了简便我们没有定义KConfig中的宏，直接将模块编译进生成的内核文件

当然这里也可以将virtio_test.o赋值给obj-m，编译成模块，启动后通过insmod进行加载virtio_test.ko

文件： drivers/virtio/Makefile

```c
obj-y += virtio_test.o
```

## 4、测试步骤及最终效果

启动后在qemu测交互打印，每次set config将会使guest内部变量自增，并通过vring发送给qemu，qemu进行打印。

![img](virtio学习笔记.assets\774036-20220209110328556-1058502921.png)

